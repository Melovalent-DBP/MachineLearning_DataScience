{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "<h1>Multivariate Linear Regression</h1>\n",
       "<b>a<sup>3</sup>x + b<sup>2</sup>x + cx + d</b>\n",
       "<p>converts to :</p>\n",
       "<b>x<sup>3</sup> = x1 , x<sup>2</sup> = x2 , x = x3 , d = c</b>\n",
       "<p>So, the equation becomes:</p>\n",
       "<b>y = m1*x1 + m2*x2 + m3*x3 + d <b>in linear regression format</b></b>\n",
       "<img src=\"./image.png\" alt=\"Multivariate Linear Regression\"><br><br><br>\n",
       "<img src=\"./MLR_2.png\" alt=\"Multivariate Linear Regression\">\n",
       "<h2>The above image good for training data but not for testing data[Overfill]</h2>\n",
       "<h3>For example: The test data graphical representaion can be:\n",
       "<img src=\"./MLR_3.png\" alt=\"Multivariate Linear Regression\"></h3>\n",
       "<br><br>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the California Data.\n",
      "Data is the features of the houses.\n",
      "Target is the price of the houses.\n",
      "Features:  ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "Performing Linear Regression.\n",
      "Train Score:  0.6109633715458154\n",
      "Test Score:  0.5911695436410466\n",
      "\n",
      "\n",
      "Adding one more feature to the data , and then performing Linear Regression.\n",
      "The new feature is the square of features.\n",
      "\n",
      "\n",
      "Train Score:  0.6109886245802101\n",
      "Test Score:  0.5911296947843678\n",
      "We can conclude that training score can be better in case of multivariate linear regression , but the test score can be worse.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h1>Complexity analysis of Normal Equation</h1>\n",
       "<h2> y = m1*x1 + m2*x2 + m3*x3 + ... + mn*xn + d</h2>\n",
       "<img src=\"./N1.png\" alt=\"Normal Equation\"><br><br>\n",
       "<img src=\"./N2.png\" alt=\"Normal Equation\"><br><br>\n",
       "<img src=\"./N3.png\" alt=\"Normal Equation\"><br><br>\n",
       "<img src=\"./N4.png\" alt=\"Normal Equation\"><br><br>\n",
       "<h1>Gradient Descent</h1>\n",
       "<h2>It is a way to find out optimal coefficients for reducing the cost function.</h2>\n",
       "<img src=\"./gd.png\" alt=\"Gradient Descent\"><br><br>\n",
       "<img src=\"./gd2.png\" alt=\"Gradient Descent\"><br><br>\n",
       "<img src=\"./gd3.png\" alt=\"Gradient Descent\"><br><br>\n",
       "<img src=\"./gd4.png\" alt=\"Gradient Descent\"><br><br>\n",
       "<h2> &alpha;: [Learning Rate] controls the rate at which movement is done in the direction of gradient.</h2>\n",
       "<img src=\"./gd5.png\" alt=\"Gradient Descent\"><br><br>\n",
       "<h2>The target is to make adaptive learning rate , so that the learning rate is not too high or too low.</h2>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coding the Gradient Descent Algorithm.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <h1> Learning rate proportional:\n",
       "    <h2>We don't need >10 iterations . Upto 10 is enough</h2>\n",
       "    <h2>We don't need >0.0001 learning_rate . Upto 0.001 learning_rate is enough</h2>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  1484.5865574086486\n",
      "1 Cost:  457.8542575737672\n",
      "2 Cost:  199.5099857255389\n",
      "3 Cost:  134.50591058200533\n",
      "4 Cost:  118.1496934223995\n",
      "5 Cost:  114.0341490603815\n",
      "6 Cost:  112.99857731713657\n",
      "7 Cost:  112.73798187568467\n",
      "8 Cost:  112.6723843590911\n",
      "9 Cost:  112.65585181499745\n",
      "1.47741737554838 0.029639347874732384\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h1>Generic Gradient Descendent</h1>\n",
       "<h2>Instead of using m*1 input , we will use m*n input <b>[m rows and n features]</b></h2>\n",
       "<h2>Hypothesis function: h() = m1x1 + m2x2 + m3x3 + ... + mn+1xn+1<b>[for c , extra n+1 is taken]</b> </h2><br><br>\n",
       "<img src=\"./GenericGD2.png\" alt=\"GenericGD\"><br><br>\n",
       "<h2><b>jth row and ith feature</b><br>\n",
       "<img src=\"./GGD3.png\"><br><br>\n",
       "<h2>cost function</h2><br>\n",
       "<img src=\"./GGD5.png\"><br>\n",
       "<img src=\"./GGD6.png\"><br><br>\n",
       "<h1>Types of Gradient Descendent</h1>\n",
       "<h2>\n",
       "<ul>\n",
       "<li>Batch GD</li>\n",
       "<li>Stochastic GD</li>\n",
       "<li>Mini Batch GD</li>\n",
       "</ul>\n",
       "</h2>\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "html_code = \"\"\"\n",
    "\n",
    "<h1>Multivariate Linear Regression</h1>\n",
    "<b>a<sup>3</sup>x + b<sup>2</sup>x + cx + d</b>\n",
    "<p>converts to :</p>\n",
    "<b>x<sup>3</sup> = x1 , x<sup>2</sup> = x2 , x = x3 , d = c</b>\n",
    "<p>So, the equation becomes:</p>\n",
    "<b>y = m1*x1 + m2*x2 + m3*x3 + d <b>in linear regression format</b></b>\n",
    "<img src=\"./image.png\" alt=\"Multivariate Linear Regression\"><br><br><br>\n",
    "<img src=\"./MLR_2.png\" alt=\"Multivariate Linear Regression\">\n",
    "<h2>The above image good for training data but not for testing data[Overfill]</h2>\n",
    "<h3>For example: The test data graphical representaion can be:\n",
    "<img src=\"./MLR_3.png\" alt=\"Multivariate Linear Regression\"></h3>\n",
    "<br><br>\n",
    "<img src=\"\n",
    "\"\"\"\n",
    "display(HTML(html_code))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Importing the California Data.\")\n",
    "california = datasets.fetch_california_housing()\n",
    "print(\"Data is the features of the houses.\")\n",
    "X = california.data \n",
    "print(\"Target is the price of the houses.\")\n",
    "Y = california.target \n",
    "print(\"Features: \",california.feature_names)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df.columns = california.feature_names\n",
    "df.describe()\n",
    "\n",
    "X_train , X_test , Y_train , Y_test = model_selection.train_test_split(X , Y , random_state=0)\n",
    "\n",
    "\n",
    "print(\"Performing Linear Regression.\")\n",
    "alg1 = LinearRegression()\n",
    "alg1.fit(X_train , Y_train)\n",
    "scaler = StandardScaler()\n",
    "#DBP_NOTES StandardScaler = Standardize features by removing the mean and scaling to unit variance\n",
    "LinearRegression(copy_X=True , fit_intercept=True , n_jobs=1 ,)\n",
    "\n",
    "Y_pred = alg1.predict(X_test)\n",
    "train_score = alg1.score(X_train , Y_train)\n",
    "test_score = alg1.score(X_test , Y_test)\n",
    "print(\"Train Score: \",train_score)\n",
    "print(\"Test Score: \",test_score)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Adding one more feature to the data , and then performing Linear Regression.\")\n",
    "print(\"The new feature is the square of features.\")\n",
    "df[\"pop_pop\"] = df.Population ** 2\n",
    "df.describe()\n",
    "X2 = df.values\n",
    "X2_train , X2_test , Y2_train , Y2_test = model_selection.train_test_split(X2 , Y , random_state=0)\n",
    "#DBP_HIGHLIGHTES random_state=0  to ensure that the split is same everytime\n",
    "#Same rows are selected for X2 and X , so that we can check the difference in the same data\n",
    "print(\"\\n\")\n",
    "\n",
    "alg2 = LinearRegression()\n",
    "alg2.fit(X2_train , Y2_train)\n",
    "Y_pred2 = alg2.predict(X2_test)\n",
    "train_score = alg2.score(X2_train , Y2_train)\n",
    "test_score = alg2.score(X2_test , Y2_test)\n",
    "print(\"Train Score: \",train_score)\n",
    "print(\"Test Score: \",test_score)\n",
    "print(\"We can conclude that training score can be better in case of multivariate linear regression , but the test score can be worse.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "html_code2 = \"\"\"\n",
    "<h1>Complexity analysis of Normal Equation</h1>\n",
    "<h2> y = m1*x1 + m2*x2 + m3*x3 + ... + mn*xn + d</h2>\n",
    "<img src=\"./N1.png\" alt=\"Normal Equation\"><br><br>\n",
    "<img src=\"./N2.png\" alt=\"Normal Equation\"><br><br>\n",
    "<img src=\"./N3.png\" alt=\"Normal Equation\"><br><br>\n",
    "<img src=\"./N4.png\" alt=\"Normal Equation\"><br><br>\n",
    "<h1>Gradient Descent</h1>\n",
    "<h2>It is a way to find out optimal coefficients for reducing the cost function.</h2>\n",
    "<img src=\"./gd.png\" alt=\"Gradient Descent\"><br><br>\n",
    "<img src=\"./gd2.png\" alt=\"Gradient Descent\"><br><br>\n",
    "<img src=\"./gd3.png\" alt=\"Gradient Descent\"><br><br>\n",
    "<img src=\"./gd4.png\" alt=\"Gradient Descent\"><br><br>\n",
    "<h2> &alpha;: [Learning Rate] controls the rate at which movement is done in the direction of gradient.</h2>\n",
    "<img src=\"./gd5.png\" alt=\"Gradient Descent\"><br><br>\n",
    "<h2>The target is to make adaptive learning rate , so that the learning rate is not too high or too low.</h2>\n",
    "\n",
    "\"\"\"\n",
    "display(HTML(html_code2))\n",
    "\n",
    "print(\"Coding the Gradient Descent Algorithm.\")\n",
    "\n",
    "\n",
    "\n",
    "def step_gradient(points , learning_rate , m , c):\n",
    "    # DBP_NOTES        We need to get m` and c` , so that we can update m and c\n",
    "    # DBP_HIGHLIGHTES  m` = m - learning_rate * (derivative of m)\n",
    "    # DBP_HIGHLIGHTES  c` = c - learning_rate * (derivative of c)\n",
    "    # DBP_HIGHLIGHTES  Derivative of m = -(2/N) * sum(yi - m*x - c) * x)\n",
    "    # DBP_HIGHLIGHTES  Derivative of c = -(2/N) * sum(yi - m*X - c)\n",
    "    \n",
    "    m_slope = 0\n",
    "    c_slope = 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i , 0]\n",
    "        y = points[i , 1]\n",
    "        m_slope += (-2 / M) * (y - m*x -c)*x\n",
    "        c_slope += (-2 / M) * (y - m*x -c)\n",
    "    \n",
    "    new_m = m - learning_rate * m_slope\n",
    "    new_c = c - learning_rate * c_slope\n",
    "    \n",
    "    return new_m , new_c\n",
    "  \n",
    "def cost(points , m , c):\n",
    "    total_cost = 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i , 0]\n",
    "        y = points[i , 1]\n",
    "        total_cost += (1/M) * ((y - m*x - c) ** 2)    \n",
    "    return total_cost    \n",
    "    \n",
    "def gd(points , learning_rate , num_itr):\n",
    "    m = 0\n",
    "    c = 0\n",
    "    for i in range(num_itr):\n",
    "        m , c = step_gradient(points , learning_rate , m , c)\n",
    "        print(i , \"Cost: \" , cost(points , m , c))\n",
    "    return m , c\n",
    "\n",
    "\n",
    "def run():\n",
    "    data = np.loadtxt(\"data.csv\" , delimiter=\",\")\n",
    "    #DBP_NOTES delimiter means separating the data by comma\n",
    "    \n",
    "    learning_rate = 0.0001\n",
    "    num_itr = 10  \n",
    "    html_code3 = \"\"\"\n",
    "    <h1> Learning rate proportional:\n",
    "    <h2>We don't need >10 iterations . Upto 10 is enough</h2>\n",
    "    <h2>We don't need >0.0001 learning_rate . Upto 0.001 learning_rate is enough</h2>\n",
    "    \"\"\"\n",
    "    display(HTML(html_code3))\n",
    "    \n",
    "    \n",
    "    \n",
    "    m , c = gd(data , learning_rate , num_itr)\n",
    "    print(m , c)\n",
    "\n",
    "run()\n",
    "\n",
    "\n",
    "html_code4 = \"\"\"\n",
    "<h1>Generic Gradient Descendent</h1>\n",
    "<h2>Instead of using m*1 input , we will use m*n input <b>[m rows and n features]</b></h2>\n",
    "<h2>Hypothesis function: h() = m1x1 + m2x2 + m3x3 + ... + mn+1xn+1<b>[for c , extra n+1 is taken]</b> </h2><br><br>\n",
    "<img src=\"./GenericGD2.png\" alt=\"GenericGD\"><br><br>\n",
    "<h2><b>jth row and ith feature</b><br>\n",
    "<img src=\"./GGD3.png\"><br><br>\n",
    "<h2>cost function</h2><br>\n",
    "<img src=\"./GGD5.png\"><br>\n",
    "<img src=\"./GGD6.png\"><br><br>\n",
    "<h1>Types of Gradient Descendent</h1>\n",
    "<h2>\n",
    "<ul>\n",
    "<li>Batch GD</li>\n",
    "<li>Stochastic GD</li>\n",
    "<li>Mini Batch GD</li>\n",
    "</ul>\n",
    "</h2>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "display(HTML(html_code4))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
