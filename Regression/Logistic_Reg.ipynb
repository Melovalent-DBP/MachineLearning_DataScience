{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "<h1>Classification Problem</h1>\n",
       "<h2> It works when the data is discrete , suppose : y tells the email is spam or not , it will be not continuous.</h2>\n",
       "<h3>\n",
       "<ul>\n",
       "<li> Binary Classification [0/1]\n",
       "<img src=\"./bclass.png\">\n",
       "</li>\n",
       "</ul>\n",
       "<h1>Logistic Regression</h1>\n",
       "<h2>Sigmoid Function / Logisitic Regression Function h(x)</h2>\n",
       "<h3>It is function that tries to reach 0 and 1.</h3>\n",
       "<img src=\"./logreg3.png\">\n",
       "<h3> h(x) = 1 / 1 + e <sup>-g(x)</sup> or 1 / 1 + e <sup>-m<sup>T</sup>x</sup>  , x1 x2 x3 features </h3>\n",
       "<h3> g(x) = mx + c = m<sup>T</sup>x = z ; m1x1 + m2x2 + m3x3 </h3>\n",
       "<h3> s(z) = 1 / 1 + e<sup>-z</sup> , where z = m<sup>T</sup>x \n",
       "<img src=\"./Logreg2.png\">\n",
       "<h3> h(x) > 0.5 , then y<sub>pred</sub> -> 1 </h3>\n",
       "<h3> h(x) &#x2264; 0.5 , then y<sub>pred</sub> -> 0 </h3>\n",
       "<img src=\"./logreg4.png\">\n",
       "<img src=\"./logreg5.png\"><br>\n",
       "<h1>Cost Function in Logistic Regression</h1>\n",
       "<h2> h(x) = 1 / (1 + e<sup>-m<sup>T</sup>x</sup>) </h2>\n",
       "<h2> h(x) = S(m<sup>T</sup>x) = 1 / (1 + e<sup>-m<sup>T</sup>x</sup>)</h2>\n",
       "<h2> Our target is to find best fit m so that the cost in minimum\n",
       "<h2>Error Function : E(h(x) , y) = ∑<sup>m</sup><sub>i=1</sub> (y<sup>i</sup> - h(x<sup>i</sup>))<sup>2</sup> where h(x) = m<sup>T</sup>x\n",
       "<h2>Now we use Gradient Descendent concept , starting from an index and moving towards the minima.</h2>\n",
       "<h2> E(h(x<sup>i</sup>) , y<sup>i</sup>) = - log(h(x<sup>i</sup>)) if y==1 , <br>\n",
       "-log(1-h(x<sup>i</sup> )) if y==0</h2>\n",
       "<h2> if y==1 , then </h2>\n",
       "<h2> h(x) -> 1 : zero error , -log(1) = 0 </h2>\n",
       "<h2> h(x) -> 0 : ∞ error , -log(0) = ∞ </h2>\n",
       "<img src=\"./logreg6.png\">\n",
       "<h2> if y==0 , then </h2>\n",
       "<h2> h(x) -> 0 : zero error , -log(1) = 0 </h2>\n",
       "<h2> h(x) -> 1 : ∞ error , -log(0) = ∞ </h2>\n",
       "<img src=\"./logreg7.png\">\n",
       "<img src=\"./logreg8.png\">\n",
       "<h1>We need to find optimal m value from previous m<sup>T</sup>x</h1>\n",
       "<h2>Using Gradient descendent: from a particular point m<sub>j</sub> , we will update it by m<sub>j</sub> =  m<sub>j</sub> - &alpha;dE/dm<sub>j</sub></h2>\n",
       "<img src=\"./logreg9.png\">\n",
       "<h2>Derivatives solve</h2>\n",
       "<img src=\"./logreg10.png\">\n",
       "<h2>y<sup>i</sup>(m<sup>T</sup>x<sup>i</sup>) = y<sup>i</sup>∑<sub>j</sub>m<sub>j</sub>x<sup>i</sup><sub>j</sub></h2>\n",
       "<h2>x<sub>j</sub><sup>i</sup> means j<sup>th</sup> feature of i<sup>th</sup> training data.\n",
       "<img src=\"./logreg11.png\">\n",
       "<h2>dE(m)/dm<sub>j</sub> = -1/m ∑<sub>i=1</sub> (y<sup>i</sup> - h(x<sup>i</sup>))x<sub>j</sub><sup>i</sup>\n",
       "<img src=\"./logreg13.png\">\n",
       "<h2>Multiclass_Logistic_Linear_Regression</h2>\n",
       "<h3>[One Vs Rest]</h3>\n",
       "<h3> y = [cat , dog , rat] </h3>\n",
       "<h3> 1 to k independent models </h3>\n",
       "<h3>\n",
       "<p>The Models are:</p>\n",
       "<ol type = \"1\">\n",
       "<li> y = [cat_{<b>y=1</b>} , not cat_{<b>y=0</b>}] -- using sigmoid function </li>\n",
       "<li> y = [dog_{<b>y=1</b>} , not dog_{<b>y=0</b>}] -- using sigmoid function </li>\n",
       "<li> y = [rat_{<b>y=1</b>} , not rat_{<b>y=0</b>}] -- using sigmoid function </li>\n",
       "</ol>\n",
       "</h3>\n",
       "<h3>\n",
       "Model 1 -> Pr(y=cat)<br>\n",
       "Model 2 -> Pr(y=dog)<br>\n",
       "Model 3 -> Pr(y=rat)<br>\n",
       "</h3>\n",
       "<h2>Other Way of Multiclass_Logistic_Linear_Regression:[Multinomial]</h2>\n",
       "<h2>p(y=j) = e<sup>m<sub>j</sub>x</sup> / ∑<sup>k</sup><sub>i=1</sub>e<sup>m<sub>i</sub>x</sup></h2>\n",
       "<h2>We have 1 to k classes</h2>\n",
       "<h2> m<sub>1</sub>.....m<sub>j</sub>.....m<sub>k</sub>n , we are using k ndim vectors</h2>\n",
       "<img src=\"./logreg14.png\"> \n",
       " \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "html1 = \"\"\"\n",
    "\n",
    "<h1>Classification Problem</h1>\n",
    "<h2> It works when the data is discrete , suppose : y tells the email is spam or not , it will be not continuous.</h2>\n",
    "<h3>\n",
    "<ul>\n",
    "<li> Binary Classification [0/1]\n",
    "<img src=\"./bclass.png\">\n",
    "</li>\n",
    "</ul>\n",
    "<h1>Logistic Regression</h1>\n",
    "<h2>Sigmoid Function / Logisitic Regression Function h(x)</h2>\n",
    "<h3>It is function that tries to reach 0 and 1.</h3>\n",
    "<img src=\"./logreg3.png\">\n",
    "<h3> h(x) = 1 / 1 + e <sup>-g(x)</sup> or 1 / 1 + e <sup>-m<sup>T</sup>x</sup>  , x1 x2 x3 features </h3>\n",
    "<h3> g(x) = mx + c = m<sup>T</sup>x = z ; m1x1 + m2x2 + m3x3 </h3>\n",
    "<h3> s(z) = 1 / 1 + e<sup>-z</sup> , where z = m<sup>T</sup>x \n",
    "<img src=\"./Logreg2.png\">\n",
    "<h3> h(x) > 0.5 , then y<sub>pred</sub> -> 1 </h3>\n",
    "<h3> h(x) &#x2264; 0.5 , then y<sub>pred</sub> -> 0 </h3>\n",
    "<img src=\"./logreg4.png\">\n",
    "<img src=\"./logreg5.png\"><br>\n",
    "<h1>Cost Function in Logistic Regression</h1>\n",
    "<h2> h(x) = 1 / (1 + e<sup>-m<sup>T</sup>x</sup>) </h2>\n",
    "<h2> h(x) = S(m<sup>T</sup>x) = 1 / (1 + e<sup>-m<sup>T</sup>x</sup>)</h2>\n",
    "<h2> Our target is to find best fit m so that the cost in minimum\n",
    "<h2>Error Function : E(h(x) , y) = ∑<sup>m</sup><sub>i=1</sub> (y<sup>i</sup> - h(x<sup>i</sup>))<sup>2</sup> where h(x) = m<sup>T</sup>x\n",
    "<h2>Now we use Gradient Descendent concept , starting from an index and moving towards the minima.</h2>\n",
    "<h2> E(h(x<sup>i</sup>) , y<sup>i</sup>) = - log(h(x<sup>i</sup>)) if y==1 , <br>\n",
    "-log(1-h(x<sup>i</sup> )) if y==0</h2>\n",
    "<h2> if y==1 , then </h2>\n",
    "<h2> h(x) -> 1 : zero error , -log(1) = 0 </h2>\n",
    "<h2> h(x) -> 0 : ∞ error , -log(0) = ∞ </h2>\n",
    "<img src=\"./logreg6.png\">\n",
    "<h2> if y==0 , then </h2>\n",
    "<h2> h(x) -> 0 : zero error , -log(1) = 0 </h2>\n",
    "<h2> h(x) -> 1 : ∞ error , -log(0) = ∞ </h2>\n",
    "<img src=\"./logreg7.png\">\n",
    "<img src=\"./logreg8.png\">\n",
    "<h1>We need to find optimal m value from previous m<sup>T</sup>x</h1>\n",
    "<h2>Using Gradient descendent: from a particular point m<sub>j</sub> , we will update it by m<sub>j</sub> =  m<sub>j</sub> - &alpha;dE/dm<sub>j</sub></h2>\n",
    "<img src=\"./logreg9.png\">\n",
    "<h2>Derivatives solve</h2>\n",
    "<img src=\"./logreg10.png\">\n",
    "<h2>y<sup>i</sup>(m<sup>T</sup>x<sup>i</sup>) = y<sup>i</sup>∑<sub>j</sub>m<sub>j</sub>x<sup>i</sup><sub>j</sub></h2>\n",
    "<h2>x<sub>j</sub><sup>i</sup> means j<sup>th</sup> feature of i<sup>th</sup> training data.\n",
    "<img src=\"./logreg11.png\">\n",
    "<h2>dE(m)/dm<sub>j</sub> = -1/m ∑<sub>i=1</sub> (y<sup>i</sup> - h(x<sup>i</sup>))x<sub>j</sub><sup>i</sup>\n",
    "<img src=\"./logreg13.png\">\n",
    "<h2>Multiclass_Logistic_Linear_Regression</h2>\n",
    "<h3>[One Vs Rest]</h3>\n",
    "<h3> y = [cat , dog , rat] </h3>\n",
    "<h3> 1 to k independent models </h3>\n",
    "<h3>\n",
    "<p>The Models are:</p>\n",
    "<ol type = \"1\">\n",
    "<li> y = [cat_{<b>y=1</b>} , not cat_{<b>y=0</b>}] -- using sigmoid function </li>\n",
    "<li> y = [dog_{<b>y=1</b>} , not dog_{<b>y=0</b>}] -- using sigmoid function </li>\n",
    "<li> y = [rat_{<b>y=1</b>} , not rat_{<b>y=0</b>}] -- using sigmoid function </li>\n",
    "</ol>\n",
    "</h3>\n",
    "<h3>\n",
    "Model 1 -> Pr(y=cat)<br>\n",
    "Model 2 -> Pr(y=dog)<br>\n",
    "Model 3 -> Pr(y=rat)<br>\n",
    "</h3>\n",
    "<h2>Other Way of Multiclass_Logistic_Linear_Regression:[Multinomial]</h2>\n",
    "<h2>p(y=j) = e<sup>m<sub>j</sub>x</sup> / ∑<sup>k</sup><sub>i=1</sub>e<sup>m<sub>i</sub>x</sup></h2>\n",
    "<h2>We have 1 to k classes</h2>\n",
    "<h2> m<sub>1</sub>.....m<sub>j</sub>.....m<sub>k</sub>n , we are using k ndim vectors</h2>\n",
    "<img src=\"./logreg14.png\"> \n",
    " \n",
    "\"\"\"\n",
    "display(HTML(html1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
